{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Chargement du jeu de donn√©es nettoy√©\n",
    "\n",
    "Importer les biblioth√®ques n√©cessaires et charger le fichier `data_drop_duplicates.csv`,\n",
    "en convertissant automatiquement la colonne `time` en datetime. V√©rification des informations g√©n√©rales du DataFrame.\n"
   ],
   "id": "a99045b04ded00c4"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Chemin vers votre fichier nettoy√©\n",
    "file_path = 'data_drop_duplicates.csv'\n",
    "\n",
    "# Charger le CSV en convertissant la colonne 'time' en datetime d√®s le d√©but\n",
    "try:\n",
    "    df = pd.read_csv(file_path, parse_dates=['time'])\n",
    "    print(\"Fichier CSV charg√© avec succ√®s.\")\n",
    "    print(\"La colonne 'time' a √©t√© automatiquement convertie en datetime.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erreur : Le fichier '{file_path}' n'a pas √©t√© trouv√©. V√©rifiez le chemin.\")\n",
    "    # Si le fichier n'est pas trouv√©, on arr√™te le script pour √©viter des erreurs\n",
    "    exit()\n",
    "\n",
    "# Afficher les informations pour v√©rifier le type de la colonne 'time'\n",
    "print(\"\\nInformations sur le DataFrame :\")\n",
    "df.info()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üßπ Nettoyage des doublons √† la minute\n",
    "\n",
    "Nous proc√©dons ici au nettoyage d√©taill√© des doublons temporels avec trois √©tapes principales :\n",
    "\n",
    "#### üîπ √âtape 1 : D√©tection et suppression des doublons profonds\n",
    "- üîç Identifier les lignes totalement identiques (m√™me minute + m√™mes valeurs sur tous les capteurs).\n",
    "- üóëÔ∏è Supprimer ces doublons en conservant la premi√®re occurrence.\n",
    "- ‚úÖ V√©rifier que tous les doublons profonds ont bien √©t√© supprim√©s.\n",
    "\n",
    "#### üîπ √âtape 2 : Identification des quasi-doublons\n",
    "- ‚è±Ô∏è Compter le nombre d‚Äôenregistrements par minute.\n",
    "- ‚ö†Ô∏è Rep√©rer les minutes avec plusieurs valeurs diff√©rentes.\n",
    "- üìù Extraire ces lignes dans `df_quasi_duplicates` pour une analyse plus approfondie.\n",
    "\n",
    "#### üîπ √âtape 3 : Nettoyage final\n",
    "- ‚úÇÔ∏è Supprimer la colonne temporaire `time_minute`.\n",
    "- üìä V√©rifier la taille finale du DataFrame principal `df`.\n",
    "\n",
    "Cette √©tape permet de pr√©parer le DataFrame pour la **fusion conditionnelle** des mesures similaires.\n"
   ],
   "id": "75318534fe862158"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"--- D√©but du processus de nettoyage des doublons √† la minute ---\")\n",
    "print(f\"Taille initiale du DataFrame : {df.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# === √âTAPE 1 : D√âTECTION ET SUPPRESSION DES DOUBLONS PROFONDS ===\n",
    "# =============================================================================\n",
    "print(\"\\n--- √âtape 1: Traitement des doublons profonds ---\")\n",
    "\n",
    "# Cr√©ation de la colonne de travail 'time_minute'\n",
    "df['time_minute'] = df['time'].dt.floor('min')\n",
    "\n",
    "# D√©finition du sous-ensemble de colonnes pour la v√©rification\n",
    "# (la minute + tous les capteurs, en ignorant les secondes de 'time')\n",
    "columns_to_check = df.columns.drop('time').tolist()\n",
    "\n",
    "# 1a. D√©tecter le nombre de doublons profonds AVANT suppression\n",
    "num_deep_duplicates_before = df.duplicated(subset=columns_to_check).sum()\n",
    "\n",
    "if num_deep_duplicates_before > 0:\n",
    "    print(f\"Trouv√© {num_deep_duplicates_before} doublon(s) profond(s). Suppression en cours...\")\n",
    "\n",
    "    # 1b. Appliquer la suppression\n",
    "    rows_before_deep_dup_removal = len(df)\n",
    "    df.drop_duplicates(subset=columns_to_check, keep='first', inplace=True)\n",
    "    rows_after_deep_dup_removal = len(df)\n",
    "\n",
    "    # 1c. V√©rification et rapport\n",
    "    print(f\"{rows_before_deep_dup_removal - rows_after_deep_dup_removal} ligne(s) de doublons profonds ont √©t√© supprim√©es.\")\n",
    "\n",
    "    # Re-v√©rification pour s'assurer que tout a √©t√© supprim√©\n",
    "    num_deep_duplicates_after = df.duplicated(subset=columns_to_check).sum()\n",
    "    if num_deep_duplicates_after == 0:\n",
    "        print(\"‚úÖ V√©rification : Suppression des doublons profonds r√©ussie.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Erreur : Il reste des doublons profonds apr√®s la tentative de suppression.\")\n",
    "else:\n",
    "    print(\"‚úÖ Aucun doublon profond trouv√©.\")\n",
    "\n",
    "print(f\"Taille du DataFrame apr√®s l'√©tape 1 : {df.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# === √âTAPE 2 : IDENTIFICATION DES QUASI-DOUBLONS RESTANTS ===\n",
    "# =============================================================================\n",
    "print(\"\\n--- √âtape 2: Identification des minutes avec des valeurs diff√©rentes ---\")\n",
    "\n",
    "# La colonne 'time_minute' existe d√©j√†, nous pouvons la r√©utiliser.\n",
    "\n",
    "# 2a. Compter le nombre d'occurrences de chaque minute\n",
    "minute_counts = df['time_minute'].value_counts()\n",
    "\n",
    "# 2b. Identifier les minutes qui apparaissent encore plus d'une fois\n",
    "# Ce sont, par d√©finition, les cas o√π les valeurs des capteurs sont diff√©rentes.\n",
    "quasi_duplicate_minutes = minute_counts[minute_counts > 1].index\n",
    "\n",
    "if not quasi_duplicate_minutes.empty:\n",
    "    print(f\"{len(quasi_duplicate_minutes)} minutes contiennent encore des enregistrements multiples (avec des valeurs diff√©rentes).\")\n",
    "\n",
    "    # 2c. Extraire ces lignes pour une analyse future\n",
    "    df_quasi_duplicates = df[df['time_minute'].isin(quasi_duplicate_minutes)].copy()\n",
    "\n",
    "    # Trier pour la lisibilit√©\n",
    "    df_quasi_duplicates.sort_values(by=['time_minute', 'time'], inplace=True)\n",
    "\n",
    "    print(f\"Un total de {len(df_quasi_duplicates)} lignes sont concern√©es.\")\n",
    "    print(\"Ces lignes sont stock√©es dans le DataFrame 'df_quasi_duplicates' pour une analyse plus approfondie.\")\n",
    "    # display(df_quasi_duplicates.head()) # D√©commenter pour voir un aper√ßu\n",
    "else:\n",
    "    print(\"‚úÖ Aucune minute avec des enregistrements multiples ne reste.\")\n",
    "\n",
    "# =============================================================================\n",
    "# === √âTAPE 3 : NETTOYAGE FINAL ===\n",
    "# =============================================================================\n",
    "\n",
    "# Supprimer la colonne de travail 'time_minute' du DataFrame principal\n",
    "df.drop(columns=['time_minute'], inplace=True, errors='ignore')\n",
    "\n",
    "print(\"\\n--- Processus de nettoyage termin√© ---\")\n",
    "print(f\"Taille finale du DataFrame principal 'df' : {df.shape}\")"
   ],
   "id": "e18c0cc470736f9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ‚öóÔ∏è Fusion conditionnelle des enregistrements par minute\n",
    "\n",
    "Apr√®s avoir identifi√© les quasi-doublons, nous fusionnons les mesures similaires pour chaque minute en suivant une logique pr√©cise :\n",
    "\n",
    "#### üîπ √âtape 1 : Cr√©ation des groupes temporels\n",
    "- ‚è±Ô∏è Cr√©er la colonne `time_minute` (arrondie √† la minute) pour regrouper les enregistrements.\n",
    "- üî¢ Identifier toutes les colonnes num√©riques (`numeric_cols`) pour le calcul des statistiques.\n",
    "\n",
    "#### üîπ √âtape 2 : Calcul des moyennes globales\n",
    "- üìä Calculer la **moyenne globale** pour chaque capteur afin de servir de r√©f√©rence pour le seuil de fusion.\n",
    "\n",
    "#### üîπ √âtape 3 : Analyse des variations locales\n",
    "- Min et max de chaque capteur pour chaque minute multi-enregistrement.\n",
    "- Calcul de la **plage de variation** (spread) pour chaque groupe.\n",
    "- D√©finir un **seuil global** bas√© sur 20% de la moyenne globale (`marge = 0.2`).\n",
    "- D√©terminer quelles minutes sont **fusionnables** (variation ‚â§ seuil pour toutes les colonnes).\n",
    "\n",
    "#### üîπ √âtape 4 & 5 : S√©parer et traiter les donn√©es\n",
    "- üìå Minutes non touch√©es : enregistrements uniques ou groupes √† forte variation.\n",
    "- üîó Minutes √† fusionner : calcul de la moyenne pour chaque capteur et conservation du premier `time`.\n",
    "\n",
    "#### üîπ √âtape 6 : Combiner et finaliser\n",
    "- üîÑ Concat√©ner les DataFrames fusionn√©s et inchang√©s pour cr√©er `df_cleaned`.\n",
    "- üîç Trier par `time` pour la coh√©rence chronologique.\n",
    "- ‚úÖ V√©rifier et afficher le nombre de minutes encore contenant des enregistrements multiples.\n",
    "\n",
    "Cette √©tape permet de **r√©duire les doublons subtils** tout en conservant la pr√©cision et la coh√©rence des mesures.\n"
   ],
   "id": "f46e2cf7ed31818"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cr√©er la colonne 'time_minute' pour identifier les groupes\n",
    "df['time_minute'] = df['time'].dt.floor('min')\n",
    "\n",
    "marge = 0.2\n",
    "\n",
    "# Lister les colonnes num√©riques\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "\n",
    "# --- √âtape 1 : Calcul des moyennes GLOBALES (la modification cl√©) ---\n",
    "print(\"\\n--- Calcul des moyennes globales pour chaque capteur ---\")\n",
    "global_means = df[numeric_cols].mean()\n",
    "\n",
    "# --- √âtape 2 : Identifier les groupes et calculer leurs stats locales ---\n",
    "\n",
    "# On ne travaille que sur les minutes ayant plus d'un enregistrement\n",
    "minute_counts = df['time_minute'].value_counts()\n",
    "multi_record_minutes = minute_counts[minute_counts > 1].index\n",
    "df_multi = df[df['time_minute'].isin(multi_record_minutes)]\n",
    "\n",
    "# Calculer les stats MIN et MAX pour ces groupes\n",
    "stats_per_minute = df_multi.groupby('time_minute')[numeric_cols].agg(['min', 'max'])\n",
    "\n",
    "# Extraire chaque statistique pour des calculs plus clairs\n",
    "mins = stats_per_minute.xs('min', axis=1, level=1)\n",
    "maxs = stats_per_minute.xs('max', axis=1, level=1)\n",
    "\n",
    "# --- √âtape 3 : Appliquer la condition de fusion avec le seuil GLOBAL ---\n",
    "\n",
    "# Calculer la plage de variation (spread) pour chaque groupe\n",
    "spread = maxs - mins\n",
    "\n",
    "# Calculer le seuil en utilisant les moyennes GLOBALES\n",
    "# C'est la ligne de code qui change la logique !\n",
    "threshold = marge * global_means\n",
    "\n",
    "# La condition est vraie si la plage est inf√©rieure ou √©gale au seuil global.\n",
    "# Pandas aligne automatiquement le DataFrame 'spread' et la S√©rie 'threshold' par colonne.\n",
    "condition_met = spread <= threshold\n",
    "\n",
    "# Une minute est \"fusionnable\" si la condition est vraie pour TOUTES ses colonnes\n",
    "is_mergeable = condition_met.all(axis=1)\n",
    "\n",
    "# Extraire les listes de minutes pour chaque cat√©gorie\n",
    "minutes_to_merge = is_mergeable[is_mergeable].index\n",
    "minutes_to_keep = is_mergeable[~is_mergeable].index\n",
    "\n",
    "print(f\"\\nAnalyse de {len(multi_record_minutes)} minutes avec plusieurs enregistrements :\")\n",
    "print(f\"  -> {len(minutes_to_merge)} minutes seront fusionn√©es (variation < 20% de la moyenne globale).\")\n",
    "print(f\"  -> {len(minutes_to_keep)} minutes seront conserv√©es telles quelles (variation > 20% de la moyenne globale).\")\n",
    "\n",
    "# --- √âtape 4 & 5 : S√©parer et traiter les donn√©es (identique √† avant) ---\n",
    "\n",
    "# 1. Donn√©es qui ne sont pas touch√©es (enregistrements uniques ou groupes √† forte variation)\n",
    "single_record_minutes = minute_counts[minute_counts == 1].index\n",
    "minutes_untouched = single_record_minutes.union(minutes_to_keep)\n",
    "df_untouched = df[df['time_minute'].isin(minutes_untouched)].copy()\n",
    "\n",
    "# 2. Donn√©es √† fusionner en calculant leur moyenne\n",
    "df_to_merge = df[df['time_minute'].isin(minutes_to_merge)]\n",
    "\n",
    "agg_dict = {col: 'mean' for col in numeric_cols}\n",
    "agg_dict['time'] = 'first'\n",
    "df_merged = df_to_merge.groupby('time_minute').agg(agg_dict)\n",
    "\n",
    "# --- √âtape 6 : Combiner et analyser le r√©sultat (identique √† avant) ---\n",
    "\n",
    "# Pr√©parer les DataFrames pour la concat√©nation\n",
    "df_untouched.drop(columns=['time_minute'], inplace=True)\n",
    "df_merged.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Combiner pour cr√©er le DataFrame final nettoy√©\n",
    "df_cleaned = pd.concat([df_untouched, df_merged], ignore_index=True)\n",
    "df_cleaned.sort_values(by='time', inplace=True, ignore_index=True)\n",
    "\n",
    "print(f\"\\nTaille du DataFrame apr√®s fusion conditionnelle : {df_cleaned.shape}\")\n",
    "\n",
    "# Compter les \"doublons\" restants dans le nouveau DataFrame\n",
    "df_cleaned['time_minute'] = df_cleaned['time'].dt.floor('min')\n",
    "final_minute_counts = df_cleaned['time_minute'].value_counts()\n",
    "remaining_multi_records = final_minute_counts[final_minute_counts > 1]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"R√âSULTAT FINAL\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Nombre de minutes contenant encore des enregistrements multiples : {len(remaining_multi_records)}\")"
   ],
   "id": "eafc1f83330b5cd6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üïµÔ∏è‚Äç‚ôÇÔ∏è Analyse d√©taill√©e des minutes avec variation significative\n",
    "\n",
    "Apr√®s la fusion conditionnelle, certaines minutes pr√©sentent encore des **√©carts importants** entre les enregistrements.\n",
    "Cette cellule permet de les isoler et de les examiner.\n",
    "\n",
    "#### üîπ √âtape 1 : Isolation des lignes √† analyser\n",
    "- ‚è±Ô∏è Identifier les minutes restantes avec plusieurs enregistrements (`remaining_multi_records`).\n",
    "- üìÇ Filtrer le DataFrame `df_cleaned` pour ne garder que ces lignes ‚Üí `df_to_inspect`.\n",
    "\n",
    "#### üîπ √âtape 2 : Pr√©paration de l‚Äôaffichage\n",
    "- üîÄ Trier par `time_minute` et `time` pour regrouper les enregistrements d‚Äôune m√™me minute.\n",
    "- üëÄ Configurer Pandas pour afficher toutes les colonnes et une largeur suffisante pour une lecture claire.\n",
    "\n",
    "#### üîπ √âtape 3 : Affichage des r√©sultats\n",
    "- üìù Afficher le nombre total de lignes et de minutes restantes.\n",
    "- ‚ö†Ô∏è Identifier pour chaque minute les **colonnes probl√©matiques** dont l‚Äô√©cart d√©passe le seuil.\n",
    "- üóÇÔ∏è Ajouter ces informations au DataFrame `df_to_inspect` dans la colonne `problem_columns`.\n",
    "\n",
    "Cette √©tape fournit une **vue d√©taill√©e des anomalies restantes** et pr√©pare les minutes probl√©matiques pour une analyse ou un traitement compl√©mentaire.\n"
   ],
   "id": "a9c1021741bd0bb8"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# --- √âtape 1 : Isoler les lignes √† analyser ---\n",
    "\n",
    "# Obtenir la liste des timestamps (minutes) qui nous int√©ressent\n",
    "minutes_to_inspect = remaining_multi_records.index\n",
    "\n",
    "# Filtrer le DataFrame 'df_cleaned' pour ne garder que les lignes correspondant √† ces minutes\n",
    "df_to_inspect = df_cleaned[df_cleaned['time_minute'].isin(minutes_to_inspect)].copy()\n",
    "\n",
    "# --- √âtape 2 : Pr√©parer l'affichage pour une meilleure lisibilit√© ---\n",
    "\n",
    "# Trier les lignes pour que les enregistrements d'une m√™me minute soient regroup√©s\n",
    "df_to_inspect.sort_values(by=['time_minute', 'time'], inplace=True)\n",
    "\n",
    "# Pour s'assurer que toutes les 92 colonnes sont visibles et non tronqu√©es\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000) # Augmenter la largeur d'affichage\n",
    "\n",
    "# --- √âtape 3 : Afficher le r√©sultat ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AFFICHAGE DES LIGNES POUR LES MINUTES AVEC UNE VARIATION SIGNIFICATIVE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Voici les {len(df_to_inspect)} lignes correspondant aux {len(remaining_multi_records)} minutes restantes :\")\n",
    "\n",
    "# --- √âtape A (Rappel de la logique) : D√©terminer les colonnes probl√©matiques par minute ---\n",
    "# (Ces variables 'spread' et 'threshold' doivent √™tre disponibles depuis le script pr√©c√©dent)\n",
    "is_problematic = spread > threshold\n",
    "\n",
    "# --- √âtape B : Pour chaque minute, cr√©er une liste des noms de colonnes probl√©matiques ---\n",
    "# C'est la ligne cl√© : elle transforme la grille de True/False en une S√©rie de listes de noms\n",
    "problematic_cols_per_minute = is_problematic.apply(lambda row: row.index[row].tolist(), axis=1)\n",
    "\n",
    "# --- √âtape C : Ajouter cette information au DataFrame d'inspection ---\n",
    "# On utilise .map() pour assigner la liste de colonnes √† chaque ligne en fonction de sa minute\n",
    "df_to_inspect['problem_columns'] = df_to_inspect['time_minute'].map(problematic_cols_per_minute)\n",
    "\n",
    "display(df_to_inspect)"
   ],
   "id": "1190be796c7be6f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üîç Analyse contextuelle des groupes probl√©matiques\n",
    "\n",
    "Cette √©tape permet d‚Äôexaminer en d√©tail les **groupes de deux lignes pr√©sentant une variation significative** pour chaque minute probl√©matique.\n",
    "\n",
    "#### üîπ V√©rifications initiales\n",
    "- ‚ö†Ô∏è S‚Äôassurer que les DataFrames `df_cleaned` et `df_to_inspect` existent avant de commencer.\n",
    "- üî¢ Identifier les colonnes num√©riques √† analyser.\n",
    "\n",
    "#### üîπ √âtape 1 : Calcul des moyennes globales\n",
    "- üìä Calculer **une seule fois** les moyennes globales pour chaque capteur (`global_means`) afin de les utiliser comme r√©f√©rence dans la comparaison.\n",
    "\n",
    "#### üîπ √âtape 2 : Parcourir chaque groupe probl√©matique\n",
    "- ‚è±Ô∏è Boucler sur chaque minute ayant deux enregistrements significatifs.\n",
    "- ‚ùå Ignorer les groupes avec un nombre de lignes diff√©rent de 2.\n",
    "\n",
    "#### üîπ √âtape 3 : Calcul des moyennes avant et apr√®s\n",
    "- üîπ Pour chaque ligne du groupe, calculer la moyenne des 10 lignes pr√©c√©dentes (`moyenne_avant`).\n",
    "- üîπ Calculer la moyenne des 10 lignes suivantes (`moyenne_apres`).\n",
    "- Ces statistiques locales servent de **contexte** pour √©valuer les √©carts.\n",
    "\n",
    "#### üîπ √âtape 4 : Pr√©parer le DataFrame de comparaison\n",
    "- üìù Ajouter les lignes probl√©matiques, les moyennes avant/apr√®s et les moyennes globales dans un DataFrame unique (`comparison_df`).\n",
    "\n",
    "#### üîπ √âtape 5 : Calculer les diff√©rences absolues\n",
    "- ‚ûñ Diff√©rences entre chaque ligne et les moyennes avant/apr√®s pour les colonnes num√©riques.\n",
    "- üîó Concat√©ner ces diff√©rences au DataFrame principal pour visualiser les √©carts.\n",
    "\n",
    "#### üîπ √âtape 6 : Affichage final\n",
    "- üëÄ Afficher uniquement les colonnes probl√©matiques et la colonne `time`.\n",
    "- üñ•Ô∏è Fournir une vue claire de la variation par rapport aux contextes global et local pour chaque minute analys√©e.\n",
    "\n",
    "Cette analyse fournit un **diagnostic pr√©cis** des anomalies et pr√©pare le DataFrame pour d‚Äô√©ventuelles corrections ou d√©cisions.\n"
   ],
   "id": "771b02eb07e7d36d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if 'df_cleaned' not in locals() or 'df_to_inspect' not in locals():\n",
    "    print(\"Erreur : Les DataFrames 'df_cleaned' ou 'df_to_inspect' n'ont pas √©t√© trouv√©s.\")\n",
    "else:\n",
    "    numeric_cols = df_cleaned.select_dtypes(include=np.number).columns.drop('time_minute', errors='ignore')\n",
    "\n",
    "    # =============================================================================\n",
    "    # === NOUVELLE √âTAPE : CALCULER LES MOYENNES GLOBALES UNE SEULE FOIS ===\n",
    "    # =============================================================================\n",
    "    # On le fait avant la boucle pour √™tre plus efficace\n",
    "    global_means = df_cleaned[numeric_cols].mean()\n",
    "    global_means.name = 'moyenne_globale_colonne'\n",
    "    # =============================================================================\n",
    "\n",
    "    grouped_problems = df_to_inspect.groupby('time_minute')\n",
    "    print(f\"D√©but de l'analyse contextuelle pour {len(grouped_problems)} groupes probl√©matiques...\")\n",
    "\n",
    "    for minute, group in grouped_problems:\n",
    "        if len(group) != 2:\n",
    "            print(f\"\\n--- Le groupe √† {minute} a {len(group)} lignes, il est ignor√©. ---\")\n",
    "            continue\n",
    "\n",
    "        # --- √âtapes 1 √† 3 (inchang√©es) ---\n",
    "        ligne1 = group.iloc[0]\n",
    "        ligne2 = group.iloc[1]\n",
    "        pos1 = df_cleaned.index.get_loc(ligne1.name)\n",
    "        pos2 = df_cleaned.index.get_loc(ligne2.name)\n",
    "\n",
    "        start_before = max(0, pos1 - 10)\n",
    "        df_before = df_cleaned.iloc[start_before:pos1]\n",
    "        moyenne_avant = df_before[numeric_cols].mean() if not df_before.empty else pd.Series(np.nan, index=numeric_cols)\n",
    "        moyenne_avant.name = 'moyenne_10_avant'\n",
    "\n",
    "        start_after = pos2 + 1\n",
    "        end_after = min(len(df_cleaned), start_after + 10)\n",
    "        df_after = df_cleaned.iloc[start_after:end_after]\n",
    "        moyenne_apres = df_after[numeric_cols].mean() if not df_after.empty else pd.Series(np.nan, index=numeric_cols)\n",
    "        moyenne_apres.name = 'moyenne_10_apres'\n",
    "\n",
    "        # --- MODIFICATION DE L'√âTAPE 4 ---\n",
    "        ligne1.name = f\"ligne_{ligne1.name} (problem)\"\n",
    "        ligne2.name = f\"ligne_{ligne2.name} (problem)\"\n",
    "        # On ajoute la ligne 'global_means' au d√©but du DataFrame de comparaison\n",
    "        comparison_df = pd.DataFrame([global_means, moyenne_avant, ligne1, ligne2, moyenne_apres])\n",
    "\n",
    "        # --- √âtape 5 (inchang√©e) : Calculer les diff√©rences ---\n",
    "        ligne1_numeric = ligne1[numeric_cols]\n",
    "        ligne2_numeric = ligne2[numeric_cols]\n",
    "        diff_l1_vs_avant = abs(ligne1_numeric - moyenne_avant); diff_l1_vs_avant.name = 'abs_diff(L1-Avant)'\n",
    "        diff_l2_vs_avant = abs(ligne2_numeric - moyenne_avant); diff_l2_vs_avant.name = 'abs_diff(L2-Avant)'\n",
    "        diff_l1_vs_apres = abs(ligne1_numeric - moyenne_apres); diff_l1_vs_apres.name = 'abs_diff(L1-Apres)'\n",
    "        diff_l2_vs_apres = abs(ligne2_numeric - moyenne_apres); diff_l2_vs_apres.name = 'abs_diff(L2-Apres)'\n",
    "        diff_df = pd.DataFrame([diff_l1_vs_avant, diff_l2_vs_avant, diff_l1_vs_apres, diff_l2_vs_apres])\n",
    "\n",
    "        final_comparison_df = pd.concat([comparison_df, diff_df])\n",
    "\n",
    "        # --- √âtape 6 (inchang√©e) : Affichage ---\n",
    "        problem_cols = ligne1['problem_columns']\n",
    "        display_cols = ['time'] + problem_cols\n",
    "        display_cols = [col for col in display_cols if col in final_comparison_df.columns]\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"ANALYSE DU GROUPE √Ä {minute.strftime('%Y-%m-%d %H:%M')}\")\n",
    "        print(f\"Colonnes avec variation significative : {problem_cols}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "        display(final_comparison_df[display_cols])"
   ],
   "id": "9c77ffb98d2818fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ‚öôÔ∏è Correction automatique et analyse des cas non r√©solus\n",
    "\n",
    "Cette √©tape combine deux phases principales pour g√©rer les **doublons de mesures par minute**.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ Phase 1 : Correction automatique\n",
    "- üü¢ Objectif : Identifier automatiquement la ligne √† supprimer lorsque deux enregistrements pr√©sentent une variation minime par rapport au contexte.\n",
    "- ‚öôÔ∏è Calcul de :\n",
    "  - Moyennes globales des colonnes num√©riques (`global_means`).\n",
    "  - Moyennes locales sur 10 lignes avant et 5 lignes apr√®s chaque groupe probl√©matique.\n",
    "- üìå Crit√®re de suppression :\n",
    "  - Si la diff√©rence minimale entre une ligne et le contexte est inf√©rieure √† 30‚ÄØ% de la moyenne globale pour cette colonne.\n",
    "- üóëÔ∏è Les lignes identifi√©es sont supprim√©es de `df_corrected`.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ Phase 2 : Affichage d√©taill√© des cas non r√©solus\n",
    "- üîç Objectif : Analyser les groupes qui restent probl√©matiques apr√®s la correction automatique.\n",
    "- üìä Pour chaque groupe restant :\n",
    "  - Calcul des moyennes locales avant/apr√®s.\n",
    "  - Affichage des lignes probl√©matiques et des colonnes pr√©sentant des variations significatives.\n",
    "  - Comparaison avec la moyenne globale pour rep√©rer les anomalies.\n",
    "- üëÄ R√©sultat :\n",
    "  - Identification pr√©cise des lignes √† examiner manuellement.\n",
    "  - Vue contextualis√©e pour chaque minute, facilitant la prise de d√©cision.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ Cette double approche permet de **corriger automatiquement la majorit√© des doublons** tout en offrant un diagnostic clair pour les cas n√©cessitant une intervention manuelle.\n"
   ],
   "id": "9979f370ba29825a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if 'df_cleaned' not in locals() or 'df_to_inspect' not in locals():\n",
    "    print(\"Erreur : Les DataFrames requis n'ont pas √©t√© trouv√©s.\")\n",
    "else:\n",
    "    # --- D√©finitions communes ---\n",
    "    marge_diff = 0.3\n",
    "    numeric_cols = df_cleaned.select_dtypes(include=np.number).columns.drop('time_minute', errors='ignore')\n",
    "    global_means = df_cleaned[numeric_cols].mean()\n",
    "\n",
    "    # =============================================================================\n",
    "    # === PHASE 1 : CORRECTION AUTOMATIQUE ===\n",
    "    # =============================================================================\n",
    "    df_corrected = df_cleaned.copy()\n",
    "    grouped_problems_for_correction = df_to_inspect.groupby('time_minute')\n",
    "    indexes_to_delete = []\n",
    "\n",
    "    print(\"--- PHASE 1: D√©but du processus de correction automatique ---\")\n",
    "    for minute, group in grouped_problems_for_correction:\n",
    "        if len(group) != 2: continue\n",
    "\n",
    "        ligne1, original_index1 = group.iloc[0], group.iloc[0].name\n",
    "        ligne2, original_index2 = group.iloc[1], group.iloc[1].name\n",
    "        pos1 = df_cleaned.index.get_loc(original_index1)\n",
    "        pos2 = df_cleaned.index.get_loc(original_index2)\n",
    "\n",
    "        moyenne_avant = df_cleaned.iloc[max(0, pos1 - 10):pos1][numeric_cols].mean()\n",
    "        moyenne_apres = df_cleaned.iloc[pos2 + 1:min(len(df_cleaned), pos2 + 6)][numeric_cols].mean()\n",
    "\n",
    "        diff_df = pd.DataFrame({\n",
    "            'abs_diff(L1-Avant)': abs(ligne1[numeric_cols] - moyenne_avant),\n",
    "            'abs_diff(L2-Avant)': abs(ligne2[numeric_cols] - moyenne_avant),\n",
    "            'abs_diff(L1-Apres)': abs(ligne1[numeric_cols] - moyenne_apres),\n",
    "            'abs_diff(L2-Apres)': abs(ligne2[numeric_cols] - moyenne_apres)\n",
    "        }).transpose()\n",
    "\n",
    "        problem_cols = ligne1['problem_columns']\n",
    "        problem_cols_in_diff = [col for col in problem_cols if col in diff_df.columns]\n",
    "        if not problem_cols_in_diff: continue\n",
    "\n",
    "        unstacked_diffs = diff_df[problem_cols_in_diff].unstack()\n",
    "        min_loc = unstacked_diffs.idxmin()\n",
    "        min_col_name, min_row_name = min_loc\n",
    "        min_diff_value = unstacked_diffs[min_loc]\n",
    "\n",
    "        threshold_value = marge_diff * global_means[min_col_name]\n",
    "\n",
    "        if min_diff_value < threshold_value:\n",
    "            if 'L1' in min_row_name: indexes_to_delete.append(original_index2)\n",
    "            else: indexes_to_delete.append(original_index1)\n",
    "\n",
    "    if indexes_to_delete:\n",
    "        print(f\"--- {len(indexes_to_delete)} lignes ont √©t√© identifi√©es pour suppression. ---\")\n",
    "        df_corrected.drop(index=indexes_to_delete, inplace=True)\n",
    "    else:\n",
    "        print(\"--- Aucune ligne n'a √©t√© supprim√©e automatiquement. ---\")\n",
    "\n",
    "    # =============================================================================\n",
    "    # === PHASE 2 : AFFICHAGE D√âTAILL√â DES CAS NON R√âSOLUS ===\n",
    "    # =============================================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASE 2: ANALYSE D√âTAILL√âE DES CAS NON R√âSOLUS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Identifier les minutes qui posent encore probl√®me DANS LE DATAFRAME CORRIG√â\n",
    "    df_corrected['time_minute'] = df_corrected['time'].dt.floor('min')\n",
    "    final_counts = df_corrected['time_minute'].value_counts()\n",
    "    remaining_problems_series = final_counts[final_counts > 1]\n",
    "\n",
    "    if remaining_problems_series.empty:\n",
    "        print(\"‚úÖ Tous les cas de doublons ont √©t√© r√©solus automatiquement !\")\n",
    "    else:\n",
    "        print(f\"{len(remaining_problems_series)} minute(s) contiennent encore des enregistrements multiples.\")\n",
    "\n",
    "        # Pour l'analyse, on retourne chercher les informations compl√®tes dans df_to_inspect\n",
    "        remaining_to_inspect = df_to_inspect[df_to_inspect['time_minute'].isin(remaining_problems_series.index)]\n",
    "        grouped_remaining_for_display = remaining_to_inspect.groupby('time_minute')\n",
    "\n",
    "        for minute, group in grouped_remaining_for_display:\n",
    "            if len(group) != 2: continue\n",
    "\n",
    "            # Ici, on utilise le code d'affichage que vous avez fourni, qui fonctionne\n",
    "            ligne1 = group.iloc[0]\n",
    "            ligne2 = group.iloc[1]\n",
    "            pos1 = df_cleaned.index.get_loc(ligne1.name)\n",
    "            pos2 = df_cleaned.index.get_loc(ligne2.name)\n",
    "\n",
    "            start_before = max(0, pos1 - 10)\n",
    "            df_before = df_cleaned.iloc[start_before:pos1]\n",
    "            moyenne_avant = df_before[numeric_cols].mean() if not df_before.empty else pd.Series(np.nan, index=numeric_cols)\n",
    "            moyenne_avant.name = 'moyenne_10_avant'\n",
    "\n",
    "            start_after = pos2 + 1\n",
    "            end_after = min(len(df_cleaned), start_after + 10)\n",
    "            df_after = df_cleaned.iloc[start_after:end_after]\n",
    "            moyenne_apres = df_after[numeric_cols].mean() if not df_after.empty else pd.Series(np.nan, index=numeric_cols)\n",
    "            moyenne_apres.name = 'moyenne_10_apres'\n",
    "\n",
    "            global_means.name = 'moyenne_globale_colonne'\n",
    "            ligne1.name = f\"ligne_{ligne1.name} (problem)\"\n",
    "            ligne2.name = f\"ligne_{ligne2.name} (problem)\"\n",
    "            comparison_df = pd.DataFrame([global_means, moyenne_avant, ligne1, ligne2, moyenne_apres])\n",
    "\n",
    "            ligne1_numeric = ligne1[numeric_cols]\n",
    "            ligne2_numeric = ligne2[numeric_cols]\n",
    "            diff_l1_vs_avant = abs(ligne1_numeric - moyenne_avant); diff_l1_vs_avant.name = 'abs_diff(L1-Avant)'\n",
    "            diff_l2_vs_avant = abs(ligne2_numeric - moyenne_avant); diff_l2_vs_avant.name = 'abs_diff(L2-Avant)'\n",
    "            diff_l1_vs_apres = abs(ligne1_numeric - moyenne_apres); diff_l1_vs_apres.name = 'abs_diff(L1-Apres)'\n",
    "            diff_l2_vs_apres = abs(ligne2_numeric - moyenne_apres); diff_l2_vs_apres.name = 'abs_diff(L2-Apres)'\n",
    "            diff_df = pd.DataFrame([diff_l1_vs_avant, diff_l2_vs_avant, diff_l1_vs_apres, diff_l2_vs_apres])\n",
    "\n",
    "            final_comparison_df = pd.concat([comparison_df, diff_df])\n",
    "\n",
    "            # Cette partie est maintenant s√ªre car 'ligne1' vient de 'df_to_inspect'\n",
    "            problem_cols = ligne1['problem_columns']\n",
    "            display_cols = ['time'] + problem_cols\n",
    "            display_cols = [col for col in display_cols if col in final_comparison_df.columns]\n",
    "\n",
    "            print(\"\\n\" + \"-\"*80)\n",
    "            print(f\"ANALYSE DU GROUPE √Ä {minute.strftime('%Y-%m-%d %H:%M')}\")\n",
    "            print(f\"Colonnes avec variation significative : {problem_cols}\")\n",
    "            print(\"-\"*80)\n",
    "            pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "            display(final_comparison_df[display_cols])"
   ],
   "id": "d74bb6d38d5f56d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üåä Lissage final et rapport des doublons restants\n",
    "\n",
    "Cette √©tape vise √† traiter les **cas restants non r√©solus** apr√®s les phases de correction automatique.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ Phase 3 : Tentative de lissage\n",
    "- üéØ Objectif : Fusionner les lignes probl√©matiques lorsque le **contexte avant/apr√®s est stable**.\n",
    "- üìù M√©thodologie :\n",
    "  - Calcul des moyennes locales **avant** et **apr√®s** chaque minute probl√©matique.\n",
    "  - V√©rification de la stabilit√© du contexte : diff√©rence entre moyennes < 20‚ÄØ% de la moyenne globale (`marge * global_means`).\n",
    "  - Si stable :\n",
    "    - Les deux lignes sont fusionn√©es.\n",
    "    - Valeur de chaque colonne probl√©matique remplac√©e par la moyenne du contexte.\n",
    "    - Suppression de la ligne redondante.\n",
    "  - Si instable :\n",
    "    - Les deux lignes sont conserv√©es pour √©viter de d√©former les donn√©es.\n",
    "\n",
    "- ‚úÖ B√©n√©fice : r√©duit manuellement les doublons tout en respectant la coh√©rence temporelle.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ Phase 4 : Rapport final\n",
    "- üìä V√©rification du nombre de minutes restant avec des doublons apr√®s lissage.\n",
    "- ‚úÖ R√©sultat attendu : **tous les doublons corrig√©s ou liss√©s**, sauf quelques cas complexes √† conserver pour analyse.\n",
    "- ‚ö†Ô∏è Les cas complexes sont affich√©s pour examen manuel.\n",
    "\n",
    "---\n",
    "\n",
    "üí° Cette approche en plusieurs phases permet un **nettoyage tr√®s fin** des donn√©es temporelles tout en pr√©servant l'int√©grit√© des mesures.\n"
   ],
   "id": "31e7557a466f3eec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if 'df_cleaned' not in locals() or 'df_to_inspect' not in locals():\n",
    "    print(\"Erreur : Les DataFrames requis n'ont pas √©t√© trouv√©s.\")\n",
    "else:\n",
    "    # =============================================================================\n",
    "    # === NOUVELLE PHASE 3 : LISSAGE DES CAS RESTANTS ===\n",
    "    # =============================================================================\n",
    "    if not remaining_problems_series.empty:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PHASE 3: TENTATIVE DE LISSAGE FINAL DES CAS RESTANTS\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        updates_to_make = []\n",
    "        indexes_to_delete_phase3 = []\n",
    "\n",
    "        remaining_to_inspect = df_to_inspect[df_to_inspect['time_minute'].isin(remaining_problems_series.index)]\n",
    "        grouped_remaining_for_smoothing = remaining_to_inspect.groupby('time_minute')\n",
    "\n",
    "        for minute, group in grouped_remaining_for_smoothing:\n",
    "            if len(group) != 2: continue\n",
    "\n",
    "            ligne1 = group.iloc[0]\n",
    "            ligne2 = group.iloc[1]\n",
    "            pos1 = df_cleaned.index.get_loc(ligne1.name)\n",
    "            pos2 = df_cleaned.index.get_loc(ligne2.name)\n",
    "            moyenne_avant = df_cleaned.iloc[max(0, pos1 - 10):pos1][numeric_cols].mean()\n",
    "            moyenne_apres = df_cleaned.iloc[pos2 + 1:min(len(df_cleaned), pos2 + 11)][numeric_cols].mean()\n",
    "\n",
    "            is_context_stable = True\n",
    "            problem_cols = ligne1['problem_columns']\n",
    "            for col in problem_cols:\n",
    "                if col not in numeric_cols: continue\n",
    "                mycontext_diff = abs(moyenne_avant[col] - moyenne_apres[col])\n",
    "                mythreshold = marge * global_means[col]\n",
    "                if mycontext_diff > mythreshold:\n",
    "                    is_context_stable = False\n",
    "                    break\n",
    "\n",
    "            if is_context_stable:\n",
    "                print(f\"Groupe √† {minute.strftime('%H:%M')}: CONTEXTE STABLE -> Lissage et fusion des lignes.\")\n",
    "                index_to_update = group.index[0]\n",
    "                index_to_delete = group.index[1]\n",
    "                indexes_to_delete_phase3.append(index_to_delete)\n",
    "                for col in problem_cols:\n",
    "                    if col not in numeric_cols: continue\n",
    "                    new_value = (moyenne_avant[col] + moyenne_apres[col]) / 2\n",
    "                    updates_to_make.append((index_to_update, col, new_value))\n",
    "            else:\n",
    "                print(f\"Groupe √† {minute.strftime('%H:%M')}: CONTEXTE INSTABLE -> Conservation des deux lignes.\")\n",
    "\n",
    "        if updates_to_make:\n",
    "            for idx, col, val in updates_to_make:\n",
    "                df_corrected.loc[idx, col] = val\n",
    "\n",
    "        if indexes_to_delete_phase3:\n",
    "            df_corrected.drop(index=indexes_to_delete_phase3, inplace=True)\n",
    "            print(f\"\\n--- {len(indexes_to_delete_phase3)} ligne(s) ont √©t√© liss√©es et fusionn√©es. ---\")\n",
    "\n",
    "    # =============================================================================\n",
    "    # === PHASE 4 : RAPPORT FINAL ===\n",
    "    # =============================================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASE 4: RAPPORT FINAL APR√àS LISSAGE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    df_corrected['time_minute'] = df_corrected['time'].dt.floor('min')\n",
    "    final_final_counts = df_corrected['time_minute'].value_counts()\n",
    "    truly_remaining_problems = final_final_counts[final_final_counts > 1]\n",
    "\n",
    "    if truly_remaining_problems.empty:\n",
    "        print(\"‚úÖ Tous les cas de doublons ont √©t√© r√©solus par correction ou lissage.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è {len(truly_remaining_problems)} cas complexes subsistent et ont √©t√© conserv√©s :\")\n",
    "        display(df_corrected[df_corrected['time_minute'].isin(truly_remaining_problems.index)])"
   ],
   "id": "24fa3ad5e36652a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üîç V√©rification finale avant indexation\n",
    "\n",
    "Cette √©tape permet de **contr√¥ler qu‚Äôaucune duplication √† la minute ne subsiste** avant d‚Äôindexer le DataFrame.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ √âtape 1 : Identifier les minutes avec doublons\n",
    "- ‚è±Ô∏è Cr√©ation d'une colonne temporaire `time_minute` (temps tronqu√© √† la minute).\n",
    "- üìä Comptage du nombre d‚Äôoccurrences pour chaque minute.\n",
    "- ‚ö†Ô∏è Filtrage pour ne garder que les minutes contenant **plus d‚Äôun enregistrement**.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ √âtape 2 : Affichage des lignes probl√©matiques\n",
    "- ‚úÖ Si aucune duplication n‚Äôest trouv√©e : pr√™t pour l‚Äôindexation et la sauvegarde.\n",
    "- ‚ö†Ô∏è Si des doublons persistent :\n",
    "  - Affichage d√©taill√© des minutes et lignes concern√©es.\n",
    "  - Les cas sont conserv√©s car leur variation a √©t√© jug√©e **significative**.\n",
    "  - Classement par minute pour une lecture facile.\n",
    "  - Toutes les colonnes visibles pour l‚Äôanalyse.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ √âtapes possibles pour g√©rer les doublons restants\n",
    "1. **Agr√©gation par moyenne** : solution simple pour obtenir un index unique.\n",
    "   Exemple :\n",
    "   `df_final = df_corrected.groupby(df_corrected['time'].dt.floor('min')).mean()`\n",
    "2. **Suppression manuelle** : retirer les lignes clairement erron√©es.\n",
    "3. **Conservation pour analyse sp√©cifique** : certains √©v√©nements peuvent n√©cessiter un traitement s√©par√©.\n",
    "\n",
    "üí° Cette √©tape est cruciale pour garantir l‚Äô**int√©grit√© temporelle** avant toute manipulation ou sauvegarde finale.\n"
   ],
   "id": "a07ae0d0864450f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if 'df_corrected' not in locals():\n",
    "    print(\"Erreur : Le DataFrame 'df_corrected' n'a pas √©t√© trouv√©.\")\n",
    "else:\n",
    "    print(\"V√©rification finale du DataFrame 'df_corrected' avant indexation...\")\n",
    "\n",
    "    # Cr√©er une copie pour l'analyse\n",
    "    df_analysis = df_corrected.copy()\n",
    "\n",
    "    # =============================================================================\n",
    "    # === √âTAPE 1 : IDENTIFIER LES MINUTES CONTENANT DES DOUBLONS ===\n",
    "    # =============================================================================\n",
    "\n",
    "    # Cr√©er une colonne temporaire avec le temps tronqu√© √† la minute\n",
    "    df_analysis['time_minute'] = df_analysis['time'].dt.floor('min')\n",
    "\n",
    "    # Compter le nombre d'occurrences de chaque minute\n",
    "    minute_counts = df_analysis['time_minute'].value_counts()\n",
    "\n",
    "    # Filtrer pour ne garder que les minutes qui apparaissent plus d'une fois\n",
    "    remaining_duplicates_minutes = minute_counts[minute_counts > 1].index\n",
    "\n",
    "    # =============================================================================\n",
    "    # === √âTAPE 2 : AFFICHER LES LIGNES CORRESPONDANTES ===\n",
    "    # =============================================================================\n",
    "\n",
    "    if remaining_duplicates_minutes.empty:\n",
    "        print(\"\\n‚úÖ BONNE NOUVELLE : Aucune duplication √† la minute n'a √©t√© trouv√©e.\")\n",
    "        print(\"Vous pouvez proc√©der directement √† l'indexation et √† la sauvegarde.\")\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ATTENTION : LES LIGNES SUIVANTES PROVOQUERONT UN INDEX NON UNIQUE\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{len(remaining_duplicates_minutes)} minute(s) contiennent encore plusieurs enregistrements.\")\n",
    "        print(\"Ces cas ont √©t√© conserv√©s par votre algorithme car leur variation a √©t√© jug√©e 'significative'.\")\n",
    "\n",
    "        # Filtrer le DataFrame pour n'afficher QUE les lignes qui posent probl√®me\n",
    "        df_to_display = df_analysis[df_analysis['time_minute'].isin(remaining_duplicates_minutes)]\n",
    "\n",
    "        # Trier pour que les groupes soient bien visibles\n",
    "        df_to_display.sort_values(by=['time_minute', 'time'], inplace=True)\n",
    "\n",
    "        # Configurer l'affichage pour voir toutes les colonnes\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.width', 1000)\n",
    "\n",
    "        # Afficher le DataFrame final\n",
    "        display(df_to_display)\n",
    "\n",
    "        print(\"\\n--- Analyse et Prochaines √âtapes ---\")\n",
    "        print(\"Vous devez maintenant d√©cider comment g√©rer ces cas restants :\")\n",
    "        print(\"1. Agr√©ger par la moyenne : C'est la solution la plus simple pour obtenir un index unique.\")\n",
    "        print(\"   Ex: df_final = df_corrected.groupby(df_corrected['time'].dt.floor('min')).mean()\")\n",
    "        print(\"2. Suppression manuelle : Si une ligne est clairement une erreur, vous pouvez la supprimer par son index.\")\n",
    "        print(\"3. Conserver pour une analyse sp√©cifique : Peut-√™tre que ces √©v√©nements ne doivent pas √™tre dans le jeu de donn√©es principal, mais analys√©s s√©par√©ment.\")"
   ],
   "id": "a11a6bf65d976c5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ‚è±Ô∏è V√©rification des minutes manquantes dans l‚Äôindex temporel\n",
    "\n",
    "Cette √©tape permet de s'assurer que le **jeu de donn√©es est continu minute par minute** apr√®s nettoyage et indexation.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ √âtape 1 : Chargement des donn√©es\n",
    "- üìÇ Fichier : `data_final_cleaned_indexed.csv`\n",
    "- ‚úÖ Chargement avec `index_col=0` pour utiliser la colonne temporelle comme index.\n",
    "- ‚ÑπÔ∏è Affichage de l‚Äôintervalle temporel couvert par le jeu de donn√©es : du premier au dernier timestamp.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ √âtape 2 : V√©rification de la continuit√©\n",
    "1. üìè Cr√©ation d‚Äôun **index de r√©f√©rence attendu** :\n",
    "   - D√©but : premi√®re minute\n",
    "   - Fin : derni√®re minute\n",
    "   - Fr√©quence : 1 minute (`freq='min'`)\n",
    "2. üîç Identification des timestamps **manquants** dans le DataFrame par rapport √† l‚Äôindex de r√©f√©rence.\n",
    "3. üìä Rapport :\n",
    "   - ‚úÖ Si aucune minute manquante : le jeu de donn√©es est complet.\n",
    "   - ‚ö†Ô∏è Si des minutes manquent :\n",
    "     - Nombre total de minutes manquantes.\n",
    "     - Pourcentage de donn√©es manquantes.\n",
    "     - Aper√ßu des premi√®res minutes manquantes (jusqu‚Äô√† 10) pour inspection rapide.\n",
    "\n",
    "---\n",
    "\n",
    "üí° Cette √©tape est cruciale pour garantir que **l‚Äôanalyse chronologique ou les calculs par minute** ne seront pas fauss√©s par des interruptions dans la s√©rie temporelle.\n"
   ],
   "id": "83e2ea9a2d852e5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- √âtape 1 : Chargement des donn√©es (comme vous l'avez fait) ---\n",
    "\n",
    "file_path = 'data_final_cleaned_indexed.csv'\n",
    "try:\n",
    "    df = pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
    "    print(\"Fichier CSV charg√© avec succ√®s.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erreur : Le fichier '{file_path}' n'a pas √©t√© trouv√©.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\nLe jeu de donn√©es s'√©tend de {df.index.min()} √† {df.index.max()}.\")\n",
    "\n",
    "# =============================================================================\n",
    "# === √âTAPE 2 : V√âRIFICATION DES MINUTES MANQUANTES ===\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"V√âRIFICATION DES MINUTES MANQUANTES DANS L'INDEX TEMPOREL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Cr√©er l'index de r√©f√©rence attendu\n",
    "#    Il commence au premier timestamp, se termine au dernier, avec une fr√©quence d'une minute ('min').\n",
    "expected_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='min')\n",
    "\n",
    "# 2. Trouver les timestamps qui sont dans l'index de r√©f√©rence mais PAS dans l'index de notre DataFrame\n",
    "missing_timestamps = expected_index.difference(df.index)\n",
    "\n",
    "# 3. Afficher le rapport\n",
    "if missing_timestamps.empty:\n",
    "    print(\"‚úÖ BONNE NOUVELLE : Aucune minute manquante n'a √©t√© d√©tect√©e dans l'intervalle.\")\n",
    "    print(\"Votre jeu de donn√©es a une fr√©quence temporelle continue.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è ATTENTION : {len(missing_timestamps)} minutes manquantes ont √©t√© d√©tect√©es.\")\n",
    "\n",
    "    total_expected_minutes = len(expected_index)\n",
    "    percentage_missing = (len(missing_timestamps) / total_expected_minutes) * 100\n",
    "\n",
    "    print(f\"   - P√©riode totale attendue : {total_expected_minutes} minutes.\")\n",
    "    print(f\"   - Pourcentage de donn√©es manquantes : {percentage_missing:.2f}%\")\n",
    "\n",
    "    print(\"\\nVoici un aper√ßu des 10 premi√®res minutes manquantes :\")\n",
    "    # On affiche seulement les 10 premi√®res pour ne pas surcharger l'affichage\n",
    "    for ts in missing_timestamps[:10]:\n",
    "        print(f\"   - {ts}\")"
   ],
   "id": "363463cdcab19c30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üïµÔ∏è Analyse des p√©riodes de minutes manquantes cons√©cutives\n",
    "\n",
    "Cette √©tape permet d‚Äôidentifier **les blocs de minutes manquantes** et leur dur√©e, plut√¥t que de lister chaque minute isol√©e.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ √âtape 1 : V√©rification de l‚Äôexistence de minutes manquantes\n",
    "- Si aucune minute n‚Äôest manquante : message `Aucune minute manquante √† analyser.`\n",
    "- Sinon : passage √† l‚Äôanalyse des p√©riodes cons√©cutives.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ √âtape 2 : Identification des p√©riodes manquantes\n",
    "1. üîπ Initialisation de la premi√®re p√©riode avec le premier timestamp manquant.\n",
    "2. üîπ Parcours de tous les timestamps manquants :\n",
    "   - Si le timestamp actuel **n‚Äôest pas cons√©cutif** au pr√©c√©dent, la p√©riode pr√©c√©dente est termin√©e.\n",
    "   - Enregistrer le **d√©but**, la **fin**, et la **dur√©e** de la p√©riode.\n",
    "   - D√©marrer une **nouvelle p√©riode**.\n",
    "3. üîπ Apr√®s la boucle, ajouter la derni√®re p√©riode manquante restante.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ √âtape 3 : Pr√©parer les r√©sultats pour affichage\n",
    "- Conversion de la liste des p√©riodes en **DataFrame** (`gaps_df`).\n",
    "- Conversion de la dur√©e en **minutes enti√®res** pour plus de clart√©.\n",
    "- Tri par **dur√©e d√©croissante** pour mettre en √©vidence les plus longues interruptions.\n",
    "- R√©initialisation de l‚Äôindex pour un affichage propre.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ √âtape 4 : Interpr√©tation\n",
    "- üìä Chaque ligne du DataFrame contient :\n",
    "  - `start` : d√©but de la p√©riode manquante\n",
    "  - `end` : fin de la p√©riode manquante\n",
    "  - `duration_minutes` : dur√©e totale en minutes\n",
    "- üìù Ce tableau permet de prioriser les **p√©riodes critiques √† combler ou analyser**.\n"
   ],
   "id": "8c573f168ef15c6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Ce code s'ex√©cute si la condition 'if not missing_timestamps.empty:' est vraie ---\n",
    "\n",
    "# S'assurer que la variable 'missing_timestamps' existe depuis la cellule pr√©c√©dente.\n",
    "if 'missing_timestamps' not in locals() or missing_timestamps.empty:\n",
    "    print(\"Aucune minute manquante √† analyser.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANALYSE DES P√âRIODES DE MINUTES MANQUANTES CONS√âCUTIVES\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Liste pour stocker les informations sur chaque p√©riode (gap)\n",
    "    gaps = []\n",
    "\n",
    "    # D√©marrer la premi√®re p√©riode avec le premier timestamp manquant\n",
    "    start_of_gap = missing_timestamps[0]\n",
    "    previous_ts = missing_timestamps[0]\n",
    "\n",
    "    # Parcourir tous les timestamps manquants √† partir du deuxi√®me\n",
    "    for ts in missing_timestamps[1:]:\n",
    "        # V√©rifier si le timestamp actuel n'est PAS la minute qui suit la pr√©c√©dente\n",
    "        if ts != previous_ts + pd.Timedelta(minutes=1):\n",
    "            # Si c'est le cas, la p√©riode pr√©c√©dente est termin√©e.\n",
    "            end_of_gap = previous_ts\n",
    "            # Calculer la dur√©e de la p√©riode\n",
    "            duration = (end_of_gap - start_of_gap) + pd.Timedelta(minutes=1)\n",
    "            # Ajouter les informations √† notre liste\n",
    "            gaps.append({'start': start_of_gap, 'end': end_of_gap, 'duration_minutes': duration.total_seconds() / 60})\n",
    "\n",
    "            # D√©marrer une nouvelle p√©riode\n",
    "            start_of_gap = ts\n",
    "\n",
    "        # Mettre √† jour le timestamp pr√©c√©dent pour la prochaine it√©ration\n",
    "        previous_ts = ts\n",
    "\n",
    "    # --- Important : Ajouter la toute derni√®re p√©riode apr√®s la fin de la boucle ---\n",
    "    end_of_gap = missing_timestamps[-1]\n",
    "    duration = (end_of_gap - start_of_gap) + pd.Timedelta(minutes=1)\n",
    "    gaps.append({'start': start_of_gap, 'end': end_of_gap, 'duration_minutes': duration.total_seconds() / 60})\n",
    "\n",
    "    # --- Afficher les r√©sultats de mani√®re lisible ---\n",
    "\n",
    "    # Convertir la liste de r√©sultats en DataFrame pour un affichage et un tri faciles\n",
    "    gaps_df = pd.DataFrame(gaps)\n",
    "\n",
    "    # Convertir la dur√©e en entier pour plus de clart√©\n",
    "    gaps_df['duration_minutes'] = gaps_df['duration_minutes'].astype(int)\n",
    "\n",
    "    # Trier par dur√©e pour voir les plus longues p√©riodes manquantes en premier\n",
    "    gaps_df.sort_values(by='duration_minutes', ascending=False, inplace=True)\n",
    "\n",
    "    # R√©initialiser l'index pour un affichage propre\n",
    "    gaps_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(f\"Les {len(missing_timestamps)} minutes manquantes sont r√©parties en {len(gaps_df)} p√©riodes cons√©cutives :\")\n",
    "    display(gaps_df)"
   ],
   "id": "12776d40cffdf0db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üõ†Ô∏è R√©√©chantillonnage pour rendre les minutes manquantes explicites\n",
    "\n",
    "Cette √©tape permet de **mettre en √©vidence les minutes manquantes** en cr√©ant des lignes avec `NaN` dans le DataFrame.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ √âtape 1 : Chargement des donn√©es\n",
    "- Lecture du fichier CSV `data_final_cleaned_indexed.csv`\n",
    "- Indexation sur la colonne temporelle existante (`time`).\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ √âtape 2 : Cr√©ation d‚Äôun index temporel complet\n",
    "- On cr√©e un **index parfait** √† fr√©quence 1 minute (`freq='min'`) couvrant toute la p√©riode.\n",
    "- R√©indexation du DataFrame avec cet index :\n",
    "  - Les minutes **manquantes** sont automatiquement remplies par des lignes contenant `NaN`.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ √âtape 3 : V√©rification\n",
    "- ‚úÖ Taille du DataFrame avant et apr√®s r√©√©chantillonnage.\n",
    "- üìä Nombre de lignes contenant au moins un `NaN` = nombre de minutes manquantes.\n",
    "- Objectif : pr√©parer le jeu de donn√©es pour un **nettoyage ou interpolation ult√©rieure**.\n"
   ],
   "id": "1a75680c12328eb2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- √âtape 1 : Chargement des donn√©es ---\n",
    "file_path = 'data_final_cleaned_indexed.csv'\n",
    "df = pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
    "\n",
    "# =============================================================================\n",
    "# === √âTAPE 2 : R√â√âCHANTILLONNAGE POUR RENDRE LES TROUS EXPLICITES AVEC NaN ===\n",
    "# =============================================================================\n",
    "print(\"\\n--- Cr√©ation d'un index temporel complet ---\")\n",
    "\n",
    "# 1. Cr√©er l'index de r√©f√©rence parfait, sans aucun trou\n",
    "full_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='min')\n",
    "\n",
    "# 2. R√©-indexer le DataFrame. C'est l'√©tape cl√©.\n",
    "#    Pandas va automatiquement cr√©er des lignes remplies de NaN pour toutes les minutes manquantes.\n",
    "df_complete = df.reindex(full_index)\n",
    "\n",
    "print(f\"Taille du DataFrame avant r√©√©chantillonnage : {len(df)}\")\n",
    "print(f\"Taille du DataFrame apr√®s r√©√©chantillonnage : {len(df_complete)}\")\n",
    "\n",
    "# Compter le nombre de lignes qui contiennent au moins un NaN\n",
    "# Ce nombre devrait √™tre √©gal au nombre de minutes manquantes que vous aviez trouv√© (2880)\n",
    "num_rows_with_nan = df_complete.isnull().any(axis=1).sum()\n",
    "print(f\"{num_rows_with_nan} lignes avec des valeurs manquantes (NaN) ont √©t√© cr√©√©es, ce qui est attendu.\")\n"
   ],
   "id": "d712bb77e7d7d89c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üèÅ Finalisation et exportation du DataFrame complet\n",
    "\n",
    "Cette cellule pr√©pare le DataFrame **r√©√©chantillonn√© et compl√©t√©** pour l'analyse ou l'export final.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ √âtape 1 : Copie de travail\n",
    "- `df_complete` est copi√© dans `df_final` pour s√©curiser les manipulations finales.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ √âtape 2 : V√©rification de l'index\n",
    "- ‚úÖ V√©rifie que l'index temporel est **unique** apr√®s le r√©√©chantillonnage.\n",
    "- ‚ö†Ô∏è Une anomalie ici indiquerait un probl√®me inattendu dans l'index.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ √âtape 3 : V√©rification des valeurs manquantes\n",
    "- Comptabilise les lignes avec au moins un `NaN`.\n",
    "- Ces lignes correspondent aux **minutes initialement manquantes**, d√©sormais explicites pour analyse/interpolation.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ √âtape 4 : Exportation\n",
    "- Le DataFrame final est enregistr√© en CSV (`data_final_cleaned_indexed.csv`) avec l‚Äôindex temporel conserv√©.\n",
    "- ‚úÖ Permet de disposer d‚Äôun jeu de donn√©es **complet et pr√™t pour l‚Äôanalyse chronologique**.\n"
   ],
   "id": "b692b42f2653e5b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# === NOUVELLE CELLULE : FINALISATION ET EXPORTATION DU DATAFRAME COMPLET ===\n",
    "# =============================================================================\n",
    "\n",
    "# --- Pr√©-requis : S'assurer que 'df_complete' est disponible depuis la cellule pr√©c√©dente ---\n",
    "if 'df_complete' not in locals():\n",
    "    print(\"‚ùå Erreur : Le DataFrame 'df_complete' n'a pas √©t√© trouv√©.\")\n",
    "    print(\"Veuillez d'abord ex√©cuter la cellule de r√©√©chantillonnage.\")\n",
    "else:\n",
    "    print(\"Pr√©paration du DataFrame complet pour l'enregistrement...\")\n",
    "\n",
    "    # --- √âtape 1 : Copie de travail ---\n",
    "    # La variable df_complete est d√©j√† presque parfaite, on la copie pour la finalisation.\n",
    "    df_final = df_complete.copy()\n",
    "\n",
    "    # --- √âtape 2 : V√©rification de l'index ---\n",
    "    # Cette √©tape est toujours une bonne pratique.\n",
    "    print(\"\\nV√©rification de l'index...\")\n",
    "    if df_final.index.is_unique:\n",
    "        print(\"‚úÖ L'index temporel est unique, comme attendu apr√®s le r√©√©chantillonnage.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Erreur inattendue : L'index n'est pas unique, ce qui ne devrait pas arriver.\")\n",
    "\n",
    "    # --- √âtape 3 : V√©rification des donn√©es manquantes ---\n",
    "    # C'est une v√©rification utile √† ajouter ici\n",
    "    num_rows_with_nan = df_final.isnull().any(axis=1).sum()\n",
    "    if num_rows_with_nan > 0:\n",
    "        print(f\"‚úÖ Le DataFrame contient {num_rows_with_nan} lignes avec des NaN, repr√©sentant les minutes manquantes.\")\n",
    "    else:\n",
    "        print(\"Le DataFrame ne contient aucune valeur manquante.\")\n",
    "\n",
    "    print(\"\\nAper√ßu du DataFrame final :\")\n",
    "    display(df_final.head())\n",
    "\n",
    "    # --- √âtape 4 : Enregistrement dans un fichier CSV ---\n",
    "    output_file_path = 'data_final_cleaned_indexed.csv'\n",
    "\n",
    "    # Enregistrer le DataFrame en CSV.\n",
    "    # L'index sera automatiquement sauvegard√© car il est l'index du DataFrame.\n",
    "    try:\n",
    "        df_final.to_csv(output_file_path)\n",
    "        print(f\"\\n‚úÖ Le DataFrame final a √©t√© enregistr√© avec succ√®s dans le fichier : '{output_file_path}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Une erreur est survenue lors de l'enregistrement du fichier : {e}\")"
   ],
   "id": "45c66df1e6a5bdd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ‚úÖ Conclusion du notebook\n",
    "\n",
    "Le DataFrame a √©t√© **nettoy√©, fusionn√©, liss√©** et **r√©index√© avec toutes les minutes explicites**.\n",
    "Les doublons significatifs ont √©t√© trait√©s et les minutes manquantes sont d√©sormais repr√©sent√©es par des `NaN`, pr√™tes pour toute interpolation ou analyse temporelle.\n",
    "\n",
    "Vous pouvez maintenant passer au **prochain notebook** : [`data_cleaned_indexed.ipynb`](./data_cleaned_indexed.ipynb) pour continuer l'analyse ou la pr√©paration des donn√©es.\n"
   ],
   "id": "c300ec16e1d13c6d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
